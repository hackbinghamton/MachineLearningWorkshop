{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection with Machine Learning\n",
    "## Overview\n",
    "\n",
    "### What You'll Learn\n",
    "In this section, you'll learn\n",
    "1. How to use various scikit-learn machine learning algorithms\n",
    "2. How to select features for a real-world machine learning problem\n",
    "3. How to design a neural network that makes predictions based on our selected features\n",
    "\n",
    "### Prerequisites\n",
    "Before starting this section, you should have an understanding of\n",
    "1. [scikit-learn](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshop/blob/main/intro_ml_scikit.ipynb) and [Tensorflow](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshop/blob/main/intro_neural_networks_tf.ipynb)\n",
    "2. [Basic Python (functions, loops, lists)](https://github.com/HackBinghamton/PythonWorkshop)\n",
    "3. [Numpy and Pandas](https://github.com/HackBinghamton/DataScienceWorkshop)\n",
    "\n",
    "### Introduction\n",
    "We've all heard about fake news over the past few years. This workshop will guide you through designing a relatively primitive fake news detector based on a modified version of the [FakeNewsNet dataset](https://github.com/KaiDMML/FakeNewsNet).\n",
    "\n",
    "### Setup\n",
    "\n",
    "**PLEASE run the below two code blocks, or else the workshop won't work!**\n",
    "\n",
    "#### Package Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow\n",
    "!pip3 install sklearn\n",
    "!pip3 install python-whois\n",
    "!pip3 install pandas\n",
    "!pip3 install textstat\n",
    "!pip3 install -U textblob\n",
    "!pip3 install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "import textblob\n",
    "from textstat.textstat import textstat\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gathering data and selecting features\n",
    "### Selecting a dataset\n",
    "For the purpose of this workshop, we'll be using a modified version of the FakeNewsNet dataset. The data provided for you has been cleaned and a few features have been added. Namely, the dataset did not originally include information from ICANN WHOIS or article text.\n",
    "\n",
    "When starting a machine learning project, it is very important to select a good dataset. Your dataset should have diverse information, be well-formed (no missing data), and not have incorrect data. It should also have a lot of data points - the 350 articles used for this exercise are not a sufficiently sized dataset.\n",
    "\n",
    "### Loading the data\n",
    "Methods that load training and testing data have been provided for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fake_news_data(file_name):\n",
    "    url = \"https://raw.githubusercontent.com/HackBinghamton/MachineLearningWorkshop/master/fake_news_detection/\" + file_name\n",
    "    json_data = requests.get(url).text\n",
    "    fake_news_data = pd.read_json(json_data)\n",
    "\n",
    "    fake_news_features = fake_news_data.drop(columns=[\"is_fake\"])\n",
    "    fake_news_labels = fake_news_data[\"is_fake\"]\n",
    "\n",
    "    return fake_news_features, fake_news_labels\n",
    "\n",
    "\n",
    "def load_fake_news_training_data():\n",
    "    return load_fake_news_data(\"fakenewsnet_modified_training_set.json\")\n",
    "\n",
    "\n",
    "def load_fake_news_testing_data():\n",
    "    return load_fake_news_data(\"fakenewsnet_modified_testing_set.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_fake_news_training_data()[0].shape)\n",
    "print(load_fake_news_training_data()[0].columns)\n",
    "\n",
    "print(load_fake_news_testing_data()[0].shape)\n",
    "print(load_fake_news_testing_data()[0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new features\n",
    "Although there's a good amount of information in this dataset, not all of it is terribly useful (yet). Let's make some functions to create new features from the data we have.\n",
    "\n",
    "### New Feature: ICANN WHOIS registered country\n",
    "A lot of fake news comes from Macedonia, Panama, or from websites whose owners hide behind domain privacy services. We can add a new column that contains a 1 if a given news article's host website was registered in Macedonia, Panama, or the location is hidden by a privacy service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suspicious_country(country):\n",
    "    sus_countries = [\"MK\", \"PA\"]\n",
    "\n",
    "    return int(country in sus_countries or \"REDACTED\" in country)\n",
    "\n",
    "\n",
    "def add_suspicious_country_column(fake_news_df):\n",
    "    fake_news_df[\"is_suspicious_country\"] = fake_news_df[\"country\"].apply(lambda x: is_suspicious_country(x))\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features: Text complexity\n",
    "Professional journalists are generally much better writers than those who create fake news. If an article is easy to read, it may have been written by a professional journalist rather than a propagandist.\n",
    "\n",
    "Let's start by writing a function that measures an article's Flesch-Kincaid reading ease level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_flesch_reading_ease_column(fake_news_df):\n",
    "    fake_news_df[\"flesch_reading_ease\"] = fake_news_df[\"article_text\"].apply(\n",
    "        lambda x: (textstat.flesch_reading_ease(x))\n",
    "    )\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be helpful to determine how many difficult words the author used. Usage of more difficult words may imply higher proficiency with the English language, which may indicate the writing was done by a professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_difficult_words(article):\n",
    "    if textstat.lexicon_count(article) == 0:\n",
    "        return 0\n",
    "\n",
    "    return textstat.difficult_words(article) / textstat.lexicon_count(article)\n",
    "\n",
    "\n",
    "def add_percent_difficult_words_column(fake_news_df):\n",
    "    fake_news_df[\"percent_difficult_words\"] = fake_news_df[\"article_text\"].apply(lambda x: percent_difficult_words(x))\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Feature: Text sentiment\n",
    "Professional journalists are expected to be objective and calm in their writing. Fake news, on the other hand, is usually opinion-heavy and designed to provoke anger from its readers. Let's add two columns which calculate the article's polarity and subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(article):\n",
    "    article = textblob.TextBlob(article)\n",
    "\n",
    "    return (article.sentiment.polarity + 1) / 2, (article.sentiment.subjectivity + 1) / 2\n",
    "\n",
    "def add_sentiment_columns(fake_news_df):\n",
    "    fake_news_df[\"article_polarity\"], fake_news_df[\"article_subjectivity\"] = zip(\n",
    "        *fake_news_df[\"article_text\"].map(analyze_sentiment)\n",
    "    )\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the features\n",
    "Now that we have functionality to create new features, let's create a function that applies all of this functionality to our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(fake_news_df):\n",
    "    # Comment or uncomment these features as you see fit. More features isn't always better -\n",
    "    # sometimes, features you think are helpful might actually harm your accuracy!\n",
    "    fake_news_df = add_suspicious_country_column(fake_news_df)\n",
    "    fake_news_df = add_flesch_reading_ease_column(fake_news_df)\n",
    "    fake_news_df = add_percent_difficult_words_column(fake_news_df)\n",
    "    fake_news_df = add_sentiment_columns(fake_news_df)\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unused features\n",
    "Unprocessed, stuff like the article ID, url, title, or article text don't make much sense to ML algorithms. Let's make a function that drops this information from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(fake_news_df):\n",
    "    # Drop features we're not using for our machine learning algorithm\n",
    "    fake_news_df = fake_news_df.drop(columns=[\"id\", \"article_text\", \"country\", \"title\", \"news_url\"])\n",
    "    fake_news_df = fake_news_df.reset_index(drop=True)\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling existing features\n",
    "As we learned last week, we want to make sure our features are scaled properly. Let's scale our creation date timestamp to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_creation_dates(fake_news_df):\n",
    "    now_timestamp = datetime.datetime.now().timestamp()\n",
    "    fake_news_df[\"creation_date\"] = fake_news_df[\"creation_date\"].apply(lambda x: x / now_timestamp)\n",
    "\n",
    "    return fake_news_df\n",
    "\n",
    "\n",
    "def scale_features(fake_news_df):\n",
    "    fake_news_df = scale_creation_dates(fake_news_df)\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling EVERYTHING together\n",
    "Finally, let's write a function that does all the feature creation, deletion, and scaling for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_fake_news_data(fake_news_df):\n",
    "    fake_news_df = add_features(fake_news_df)\n",
    "    fake_news_df = drop_features(fake_news_df)\n",
    "    fake_news_df = scale_features(fake_news_df)\n",
    "\n",
    "    return fake_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training our algorithms\n",
    "### The scikit-learn approach\n",
    "Let's begin by first testing out some `scikit-learn` algorithms and observing how they perform.\n",
    "\n",
    "We see in the first few lines, \"Logistic Regression,\" \"Linear Discriminant Analysis,\" \"K-Nearest Neighbors,\" and several other complicated sounding terms. These are all different types of classifiers, and previous pages in the workshop have discussed several of these. As for any you aren't familiar with, it's most important to know that they all function differently but attempt to achieve the same goal. Some may be extremely good classifiers for the dataset, others might perform so poorly it would actually hurt the neural network to leave them in. It's good to check multiple and see how accurately they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn_models(training_features, training_labels):\n",
    "    models = [\n",
    "        (\"Logistic Regression\", LogisticRegression(solver=\"lbfgs\")),\n",
    "        (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
    "        (\"K-Nearest Neighbors\", KNeighborsClassifier()),\n",
    "        (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "        (\"Gaussian Naive Bayes\", GaussianNB()),\n",
    "        (\"Support Vector Machine\", SVC(gamma=\"scale\")),\n",
    "        (\"Bagging Classifier\", BaggingClassifier()),\n",
    "        (\"Random Forest Classifier\", RandomForestClassifier(n_estimators=100))\n",
    "    ]\n",
    "\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10)\n",
    "\n",
    "        cv_results = model_selection.cross_val_score(\n",
    "            model, training_features, training_labels, cv=kfold, scoring=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        msg = \"%s: \\n\\tAverage accuracy: %f \\n\\tStandard deviation: %f\" % (\n",
    "            name, cv_results.mean() * 100, cv_results.std() * 100\n",
    "        )\n",
    "\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing and training a neural network\n",
    "Let's now try designing a neural network.\n",
    "\n",
    "In the first few lines, we create a dense relu layer, a \"droupout layer\", and a dense softmax layer. ReLu and SoftMax are both __activation functions__ that help us process the data in each node in the given layer. A \"dropout layer\" tells the neural network to drop a certain amount of nodes to help it learn. Without this, the \"neurons\" can develop a sort of interdependency on each other, which can lead to overfitting. By forcing the network to drop some of the nodes each time it trains, it has to find new features with only the nodes it now has remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network():\n",
    "    # This is the same design as last week's neural network, with the exception that:\n",
    "    #     1. There is no input to flatten\n",
    "    #     2. The dense softmax layer has been reduced from 10 units to 2 units, since our labels \n",
    "    #        can either be true or false (2 options) as opposed to a digit between 0 and 9 (10 options)\n",
    "    dense_relu_layer = tf.keras.layers.Dense(1024, activation=\"relu\")\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.2)\n",
    "    dense_softmax_layer = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "\n",
    "    neural_network_model = tf.keras.models.Sequential([\n",
    "        dense_relu_layer,\n",
    "        dropout_layer,\n",
    "        dense_softmax_layer\n",
    "    ])\n",
    "\n",
    "    neural_network_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return neural_network_model\n",
    "\n",
    "\n",
    "def train_neural_network(neural_network_model, training_features, training_labels):\n",
    "    neural_network_model.fit(training_features.values, training_labels.values, epochs=400)\n",
    "\n",
    "    return neural_network_model\n",
    "\n",
    "\n",
    "def evaluate_neural_network(neural_network_model, testing_features, testing_labels):\n",
    "    test_loss, test_acc = neural_network_model.evaluate(testing_features.values, testing_labels.values)\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluating our algorithms\n",
    "Now that we've designed our approach to the problem, let's execute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    fake_news_training_features, fake_news_training_labels = load_fake_news_training_data()\n",
    "    fake_news_testing_features, fake_news_testing_labels = load_fake_news_testing_data()\n",
    "\n",
    "    fake_news_training_features = refine_fake_news_data(fake_news_training_features)\n",
    "    fake_news_testing_features = refine_fake_news_data(fake_news_testing_features)\n",
    "\n",
    "    evaluate_sklearn_models(fake_news_training_features, fake_news_training_labels)\n",
    "\n",
    "    neural_network_model = create_neural_network()\n",
    "    neural_network_model = train_neural_network(\n",
    "        neural_network_model, fake_news_training_features, fake_news_training_labels\n",
    "    )\n",
    "\n",
    "    print(evaluate_neural_network(neural_network_model, fake_news_testing_features, fake_news_testing_labels))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "There are many ways we can do better. Try doing the following on your own:\n",
    "1. Playing around with tensorflow parameters/adding different layers\n",
    "2. Adding new features\n",
    "\n",
    "Possibly helpful further reading:\n",
    "1. [Types of Keras layers](https://keras.io/layers/core/)\n",
    "2. [Types of Keras Activations](https://keras.io/activations/)\n",
    "3. [Fake News Detector from HackBU 2018](https://github.com/cfiutak1/HackBU2018-Fake-News-Detector/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
